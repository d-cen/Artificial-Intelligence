{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment-1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Lo68A2ltMkJZ"},"source":["# ECE 570: Assignment 1\n","\n","## **Instructions**\n","1. Please follow the thread in Piazza for detailed usage of Google Colab.\n","2. All submissions should be uploaded to Gradescope as a PDF version of your current Jupyter notebook. In this assignment you only need to submit section 3, 4, 5, and 6. \n","3. Have fun!\n"]},{"cell_type":"markdown","metadata":{"id":"O0qw691FVGPi"},"source":["## 1. Background\n","In this assignment, we are trying to do simple sentiment analysis. Sentiment analysis is the process of detecting positive or negative sentiment in text. Itâ€™s often used by businesses to detect sentiment in social data, gauge brand reputation, and understand customers.\n","\n","The dataset we will be using is called the [***Stanford Sentiment Treebank***](https://nlp.stanford.edu/sentiment/code.html). This dataset is collected from movie reviews on *Rotten Tomatoes* and includes over 20k sentences. All reviews were later re-organized as distinct phrases with a label as a number between 0.0 and 1.0. Labels can later be divided in to five intervals [0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0] which means very negative, negative, neutral, positive, and very positive, respectively.\n","\n","The dataset we are using in this assignment is a subset of Stanford Sentiment Treebank and consists of only **400 phrases**. For simplicity, the dataset only contains extreme sentiments; half of them are extremely positive reviews (have corresponding range (0.9, 1.0]), and the other half are extremely negative reviews (have corresponding range [0.0, 0.1]).\n","\n","In machine learning (ML), the process of optimizing the model given some dataset is known as *training* or *learning*---thus the data used during training is merely called the *training data* (e.g., historicial reviews). This is analagous to a student seeing worked-out problems in class to help understand a concept.  However, after the model has been trained, it will be used to predict on new data (e.g., new reviews)---this is what is often called *test data*. This process is analagous to a student taking a test and requires them to understand the concepts rather than just memorizing the problems worked-out in class. We will cover these concepts in more detail later on in the lectures.\n","\n","To *simulate* this process and evaluate your model *as if it was deployed*, we will split the dataset into two parts: a training dataset (50%) and test dataset (50%), where both have an equal number of positive and negative reviews.\n","\n","Your job is to manually construct a simple function that determines whether a single phrase (input) has a positive or negative sentiment (output/return value). You are only allowed to see and optimize using the **training dataset** (simulating model training in ML), but we provide a function that performs a final evaluation of your model on the **testing dataset** (simulating the model being deployed in the real world). \n","\n","*Note: We obfuscate (i.e., make secret) the testing function because in the real world, you won't know what the new data will look like.*"]},{"cell_type":"markdown","metadata":{"id":"IuRq9xvxNoX4"},"source":["## 2. Mounting your google drive on Colab\n","Since colab is running on a remote server on Google, you need to mount your google drive on Colab to serve as a 'local directory' to your coding environment. Luckily, it is as simple as two steps! Try to run this block and follow the instructions that got popped out.\n","\n","Note: This part is not necessary if you are using your own Python environment or other remote python environment."]},{"cell_type":"code","metadata":{"id":"pSTudka7I6gH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630624839314,"user_tz":420,"elapsed":394,"user":{"displayName":"David Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtDeHbQEUkV6A8Tj9lvd4zVI95JLcO3gEueFpS-g=s64","userId":"01592668474975148584"}},"outputId":"1404fe38-5165-4ee1-f903-0de88368ed12"},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"iKX2M2NfOvPH"},"source":["## 3. Load data (10/100 points)\n","Now, we need to load the data from the \"train.txt\" and \"test.txt\" file. Please change the location for **dir_root** in the following code block to where you saved all your files.\n","\n","The train dataset is stored in the \"train.txt\" file which stores 100 positive phrases and 100 negative phrases. Each line in the file is consist of a phrase and the corresponding sentiment positive(1) or negative(-1) followed by a separation mark '|'. \n","\n","Tip: It is helpful and sometimes necessary to have a separate folder for each assignment!"]},{"cell_type":"code","metadata":{"id":"neQW_OYmI_me"},"source":["import os                                                                       # For better path controls\n","dir_root = '/content/drive/MyDrive/ECE570_assignments/Assignment-1/'            # Change this root directory\n","train_dir = os.path.join(dir_root, 'train.txt')                                 # Locate the train.txt file"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2dxJu94JQ8Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630625332228,"user_tz":420,"elapsed":540,"user":{"displayName":"David Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtDeHbQEUkV6A8Tj9lvd4zVI95JLcO3gEueFpS-g=s64","userId":"01592668474975148584"}},"outputId":"16c22124-330e-40d9-f1ee-d94b42097d2d"},"source":["# Use built-in function \"open\" to read files \n","# Use the \"with\" syntax to automatically close the file after the block\n","with open(train_dir, 'r') as f:\n","    train_lines = f.readlines()\n","\n","# Construct two lists to store phrases and labels seperately\n","train_data, train_label = [], []\n","for line in train_lines:\n","    line_sec = line.split(\"|\", -1)\n","    train_data.append(line_sec[0])\n","    train_label.append(int(line_sec[1]))\n","\n","# Preview some data here\n","preview = 10 \n","for i, (phrase, label) in enumerate(zip(train_data[:preview], train_label[:preview])):\n","    print(f'Phrase {i:03} \\\"{phrase}\\\" has the sentiment {label}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Phrase 000 \"Astonishingly skillful and moving\" has the sentiment 1\n","Phrase 001 \"are incredibly beautiful to look at\" has the sentiment 1\n","Phrase 002 \"as the most magical and most fun family fare of this or any recent holiday season\" has the sentiment 1\n","Phrase 003 \"It shows that some studios firmly believe that people have lost the ability to think and will forgive any shoddy product as long as there 's a little girl-on-girl action .\" has the sentiment -1\n","Phrase 004 \"Will assuredly rank as one of the cleverest , most deceptively amusing comedies of the year .\" has the sentiment 1\n","Phrase 005 \"disintegrates into a dreary , humorless soap opera\" has the sentiment -1\n","Phrase 006 \"The editing is chaotic , the photography grainy and badly focused , the writing unintentionally hilarious , the direction unfocused ,\" has the sentiment -1\n","Phrase 007 \"The film is often filled with a sense of pure wonderment and excitement not often seen in today 's cinema du sarcasm\" has the sentiment 1\n","Phrase 008 \"is as appalling as any ` comedy ' to ever spill from a projector 's lens\" has the sentiment -1\n","Phrase 009 \"... could easily be called the best Korean film of 2002 .\" has the sentiment 1\n"]}]},{"cell_type":"markdown","metadata":{"id":"hH3n1eHKhGm6"},"source":["## 4. Classifier (80/100 points)\n","Please fill in code in the provided skeleton for the function `sentiment_analysis` which has the following structure:\n","* Input: a single string `phrase`\n","* Output/Return value: an integer `-1` or `1`. `-1` stands for negative sentiment and `1` stands for positive sentiment\n","\n","This task is similar to the hand-crafted knowledge phase of AI in which you are manually creating rules or performing manual computation by using your intuition and looking at some examples. You should not use any ML packages for this assignment (simple Python code should be enough).\n","\n","Notes:\n","1. To receive full credit, your training accuracy must be greater than 60%. (After some effort, Prof. Inouye was able to get 66.5%. Can you do better?)\n","2. Your code should be less than **50 lines without importing any additional packages** (i.e, this assignment does not require you to perform any complicated model analysis).  Only 10-20 lines is likely required if written concisely.\n","3. You can view all the training phrases by opening file *'train.txt'* in the provided zip file. This may help understand what could be used in your function.\n","4. Throughout the design of your algorithm, **you should only have access to the train dataset** stored in \"train.txt\". The test dataset stored in 'test.npy' is used in the next evaluation section but you should not look at this (see discussion above). Again, you can think that train dataset is what we would actually have to learn from (like course materials and lectures) while test is new data that simulates real-world posts (where we wouldnâ€™t usually know the true labels). \n","\n","You might find the following hints helpful (not required to use them):\n","1. The Python keyword `in` can be used to determine if a string is within another string. e.g., `'ece' in 'hello ece 570'` would evaluate to `True`.\n","2. The `lower()` or `upper()` methods of string can be helpful, e.g., if `a = 'HeLlo'` then `print(a.lower())` would print `hello`.\n","3. A partial frequency table for all words in the training dataset is given as the follow:\n","\n","Word | # positive | # negative | # total\n","--- | --- | --- | ---\n","best|12|0|12\n","i|0|11|11\n","are|9|1|10\n","most|9|1|10\n","bad|0|10|10\n","at|2|6|8\n","his|7|1|8\n","has|5|3|8\n","about|2|6|8\n","have|1|6|7\n","from|2|4|6\n","worst|0|6|6\n","does|2|4|6\n","brilliant|6|0|6\n","films|6|0|6\n","any|1|4|5\n","enough|1|4|5\n","what|4|1|5\n","work|5|0|5\n","great|4|1|5\n","time|1|4|5\n","or|1|3|4\n","some|1|3|4\n","will|3|1|4\n","sense|3|1|4\n","cinema|3|1|4\n","comedy|1|3|4\n","just|1|3|4\n","first|4|0|4\n","masterpiece|3|1|4\n","my|0|4|4\n","want|1|3|4\n","if|0|4|4\n","something|3|1|4\n","story|3|1|4\n","love|4|0|4\n","filmmaking|2|2|4\n","their|4|0|4\n","when|0|4|4\n","than|1|3|4\n","look|1|2|3\n","recent|3|0|3\n","product|0|3|3\n","into|0|3|3\n","hilarious|2|1|3\n","often|3|0|3\n","easily|3|0|3\n","performances|3|0|3\n","deserves|3|0|3\n","\n"]},{"cell_type":"code","metadata":{"id":"TNCrqyudK5NU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630626728334,"user_tz":420,"elapsed":580,"user":{"displayName":"David Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtDeHbQEUkV6A8Tj9lvd4zVI95JLcO3gEueFpS-g=s64","userId":"01592668474975148584"}},"outputId":"03b3a361-6a60-4fd7-848a-728a2b9d10f2"},"source":["# Import frequency table\n","freq_dir = os.path.join(dir_root, 'freq_table.txt')  \n","\n","with open(freq_dir, 'r') as f:\n","    freq_table = f.readlines()\n","\n","# Create frequency table\n","# If the positive reviews are as same or more than negative reviews\n","# then the sentiment value of the word is 1, else -1\n","freq_word, freq_sentiment_value = [], []\n","for line in freq_table:\n","    line_sec = line.split(\"|\")\n","\n","    word_name = line_sec[0]\n","    pos_num = int(line_sec[1])\n","    neg_num = int(line_sec[2])\n","\n","    freq_word.append(word_name)\n","    if (neg_num and (pos_num // neg_num) < 1):\n","      freq_sentiment_value.append(-1)\n","    else:\n","      freq_sentiment_value.append(1)\n","\n","def sentiment_analysis(phrase):\n","    \"\"\"\n","    sentiment_analysis function determines whether a phrase is positive (1) or negative (-1).\n","\n","    :param1(string) phrase: a single phrase in the format of string\n","    :return(int)          : 1 if the phrase is postive or -1 if the phrase is negative\n","    \"\"\" \n","    \n","    # Set phrase to lowercase and tokenize\n","    phrase = phrase.lower()\n","    phrase_sec = phrase.split(\" \")\n","\n","    # Match words in phrase to word in frequency table\n","    # Shouldn't use \"in\" to compare, words like \"i\" in frequency\n","    # table would affect the classifier's decision\n","    phrase_sentiment = 0\n","    for i, word in enumerate(freq_word):\n","      for phrase_word in phrase_sec:\n","        if word == phrase_word:\n","          phrase_sentiment += freq_sentiment_value[i]\n","    \n","    # Since return value has to be 1 or -1, 0 are considered positive\n","    if phrase_sentiment > -1:\n","      return 1\n","    else:\n","      return -1\n","\n","def evaluate(func, data, label):\n","    score = 0\n","    for x, y in zip(data, label):\n","        score += (func(x) == y)\n","    return score/len(data)\n","\n","train_acc = evaluate(sentiment_analysis, train_data, train_label)\n","print(f\"Your method has a training accuracy of {train_acc*100}%\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Your method has a training accuracy of 76.5%\n"]}]},{"cell_type":"markdown","metadata":{"id":"AadzsP3uRWib"},"source":["## 5. Evaluate (10/100 points)\n","You may already notice that there is an extra evaluation function in the above coding block which helps calculate the accuracy for your algorithm on the training dataset. The metric that we used to evaluate is straightforward:    \n","$$\\text{accuracy} = \\frac{\\text{number of correct prediction}}{\\text{number of total cases}}$$\n","Now, let's test the performance of your algorithm on the test dataset (as if deployed on new reviews)! \n","\n","Try to get the **test accuracy** to be higher than 55% to receive **full credit**!\n","\n","Note: You should not have the accuracy to be lower than 50%! (Why?)"]},{"cell_type":"code","metadata":{"id":"JaoWgyj2K1TR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630624839786,"user_tz":420,"elapsed":6,"user":{"displayName":"David Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtDeHbQEUkV6A8Tj9lvd4zVI95JLcO3gEueFpS-g=s64","userId":"01592668474975148584"}},"outputId":"59840941-c7c0-4048-ffb6-9147e7dd7462"},"source":["import sys\n","sys.path.append(dir_root)\n","from top_classified_file import super_secret_function\n","\n","test_dir = os.path.join(dir_root, 'test.npy')\n","test_acc = super_secret_function(test_dir, sentiment_analysis)\n","\n","print(f\"Your method has a test accuracy of {test_acc*100}%\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Your method has a test accuracy of 60.0%\n"]}]},{"cell_type":"markdown","metadata":{"id":"CdSf3f-uS5jt"},"source":["## 6. (Optional) Did you notice something interesting?\n","1. During your design, was the training accuracy always higher than test accuracy? Can you explain why that might be true?\n","\n","> The training accuracy is always higher than test accuracy for this design. This is true because we are using a frequency table based on our training data. However, the frequency table is not related to the test data. So more phrase would be misclassified.\n","\n","2. Was the sentiment analysis task harder than you expected? Why or why not?\n","\n","> No. This is easy version of Natural Language Processing. We just have to manually construct our own frequency table and compare the training data to the frequency table.\n","\n","3. Anything else you learned or feedback you may have?\n","\n","> My feedback is that if we should try stem the word, then the accuracy would increase. There are package in NLTK toolkit that could be useful to process the data better.\n","\n","## 7. (Optional) Further reading and exploration\n","If you are interested in text data, you could look into the scikit-learn tutorial on analyzing text: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html  We will discuss scikit-learn a little later in the semester."]}]}